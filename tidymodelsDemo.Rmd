---
title: "Intro to tidymodels and XGBoost in R"
author: "J. Harrison"
date: "7/22/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction
So, sklearn is a pretty cool Python package for doing machine learning. However, the tidymodels library for R seems like it may be a contender for the throne. It seems to offer similarly convenient tools for feature engineering and various data preprocessing, but relies on the tidyverse ease-of-use-aesthetic. Moreover, from what I can gather, R is a bit easier to make graphics in compared to Python (matplotlib) so staying within the R environment instead of moving from Python to R and back seems convenient. Also, Rstudio is a way better IDE than anything Python has on offer, in my opinion. Finally, I suspect saving model objects may be a bit better with tidymodels than in python. Lets find out!

As an aside, tidymodels has some really good documentation. Kudos to them for making software docs that are actually enjoyable to read and easy to understand.

# Installing all the things...

tidymodels imports a lot of stuff (see the chunk), but much of it is standard for the tidyverse (e.g., ggplot, tidyr, etc.). Packages that are specific to the tidymodels framework include the following:

* dials - a package for tuning model parameters
* parsnip - provides a common interface for models, e.g., different decision-tree model softwares may say ntree, treen, num_tree all to specify the number of trees. Remembering the differences among tools, or looking them up, is not a great use of time...enter parsnip.
* tune - also useful for tuning models, has stuff to help with pre- and post-processing data and results
* workflows - allows models and preprocessing steps to be contained in a workflow object. Analogous to pipelines in sklearn.
*workflowsets - makes a bunch of workflows
*yardstick - model evaluation tools

FYI if you are on an Apple the gower dependency probably won't install from source. The OS X compiler doesn't support openMP and so compilation will probably fail. Just install gower without compilation to get round this issue.
```{r}
#install.packages("gower")
#install.packages("tidymodels")
#Tidymodels imports all this: 
# broom (≥ 0.7.6), cli (≥ 2.4.0), conflicted (≥ 1.0.4), dials (≥ 0.0.9), dplyr (≥ 1.0.5), ggplot2 (≥ 3.3.3), infer (≥ 0.5.4), modeldata (≥ 0.1.0), parsnip (≥ 0.1.5), purrr (≥ 0.3.4), recipes (≥ 0.1.16), rlang (≥ 0.4.10), rsample (≥ 0.0.9), rstudioapi (≥ 0.13), tibble (≥ 3.1.0), tidyr (≥ 1.1.3), tune (≥ 0.1.3), workflows (≥ 0.2.2), workflowsets (≥ 0.0.2), yardstick (≥ 0.0.8)
 	
library(tidymodels)

#install.packages("xgboost")
library(xgboost)

# unleash the beast (actually, I need to learn more about this package and when it is helpful...hah)
#install.packages(doParallel)
library(doParallel)
all_cores <- parallel::detectCores(logical = FALSE)
registerDoParallel(cores = all_cores)

set.seed(666)
```
Load some data and do a bit of minor wrangling. Note that I make no effort at code efficiency here. 
```{r}
X <- read.csv("./imputed_scaled_ITS_metadata.csv")
    
#Bring in taxon proportions, these were wrangled in R after being generated by CNVRG
taxa <- read.csv("./ITSp_estimates_wrangled_for_post_modeling_analysis_divided_by_ISD.csv")
#Get most abundant taxon.
focaltaxon <- which.max(colSums(taxa[,5:(length(taxa)-4)])) #Indexing is to cut off stuff bookending the df that I don't want to consider

X$compartment <- ifelse(X$EN == 1, "EN", NA)
X$compartment[X$EP == 1] <- "EP"
table(X$compartment)

#Make sure our data are in the same order
X$sample <- paste(X$plant.x, X$compartment, sep = "_")
table(X$sample %in% taxa$sample)
#the missing stuff were things I removed due to poor sequencing
merged_dat <- merge(X, taxa, by.x = "sample", by.y = "sample")

#Making a response object for readability
response_taxon <- merged_dat[, names(merged_dat) == names(focaltaxon)]

#Get rid of stuff we don't need in merged_dat (all the bogus stuff was from merging with taxa, so we can index by og dimensions)
merged_dat <- data.frame(response_taxon,
                         merged_dat[,1:length(names(X))])
```
Let's do a simple train test split of the data, while stratifying by compartment and presence of the focal taxon. I stratify via two things by pasting them together.
```{r}
focal_one_hot <- ifelse(response_taxon > 0.00042, 1, 0)
merged_dat$stratify <- paste(merged_dat$compartment, focal_one_hot)
merged_dat_split <- rsample::initial_split(
  merged_dat, 
  prop = 0.2, 
  strata = stratify
)
```
Note that I already did a lot of feature engineering on these (imputing, etc.), so my demo of how to do preprocessing will not include some common tasks. For more, see [HERE](https://www.r-bloggers.com/2020/05/using-xgboost-with-tidymodels/) and [HERE](https://www.tidymodels.org/start/recipes/).

From the help, "A 'recipe' is a description of the steps to be applied to a data set in order to prepare it for data analysis." To initiate a recipe we use R's standard formula syntax:
```{r}
phyllo_recipe <- 
  recipe(response_taxon ~ ., data = merged_dat) 
```
This object is a tibble and looks like this
```{r}
summary(phyllo_recipe)
```
Note the column for 'role' this holds info about what the feature is for. Predictor and outcome are the critical ones, but we can add any roles we want. The help docs demonstrate a useful example where they have an "ID" role. Thus a variable that contains sample info can be retained in the dataset, but not included as a predictor. This is handy because subsetting data to remove identifiers and then putting them back in can lead to critically damning errors if one gets anything out of order. I like that one can sidestep that whole issue by just designating roles up front. Note, from my reading I do not think sklearn has anything analogous to this and I think this handy organization could be a nice time saver and a way to avoid fragility imposed via lots of indexing and splitting/merging of data.
```{r}
#adding some roles
phyllo_recipe <- 
  recipe(response_taxon ~ ., data = merged_dat) %>%
          update_role(
            names(merged_dat)[1:11], #stuff we want to recode the role of, also next two lines below
            stratify, 
            X.1.y, 
            new_role = "ID")
summary(phyllo_recipe)
```
Now all of our bookkeeping crap are coded as "ID", pretty handy!

Note that I already turned my categoricals into one-hot encoded variables, but tidymodels has some convenient ways to do this too. They do simple, helpful things like remove the original categorical variable from the dataset. Lots of convenience, particularly for putting one-hot encoded variables back together and defining interactions among them as yet more dummy coded variables. See [HERE](https://recipes.tidymodels.org/articles/Dummies.html).

Next, we need to prep to run our recipe. From prep's help: "For a recipe with at least one preprocessing operation, estimate the required parameters from a training set that can be later applied to other data sets."

If you have a recipe that requires any sort of preprocessing you will have to run prep before you move on.
```{r}
phyllo_recipe <- prep(phyllo_recipe)
```
Alright, once you have a recipe sorted and your ingredients prepped you got to put it in the oven! Seriously, the command used to execute a recipe is called bake.
```{r}
phyllo_chopped <- bake(
    phyllo_recipe, 
    new_data = merged_dat_split
  ) %>%  
  rsample::vfold_cv(v = 5)
```
Alright, this is starting to smell tasty (to continue the analogy to an unfortunate degree). Next, we define the model. Note that we use parsnip to do this. Recall that parsnip is a part of the tidymodels ecoystem that provides a standard API for various models. All the calls to tune() are the model parameters that we will want to tune. During the set_engine command we are saying that we want to use xgboost to run our model and we want squared error to be our measure of success. For different engines you can pass in specific options as needed. 

The next few chunks were lifted from [HERE](https://www.r-bloggers.com/2020/05/using-xgboost-with-tidymodels/).
```{r}
xgboost_model <- 
  parsnip::boost_tree(
    mode = "regression",
    trees = 1000,
    min_n = tune(),
    tree_depth = tune(),
    learn_rate = tune(),
    loss_reduction = tune()
  ) %>%
    set_engine("xgboost", objective = "reg:squarederror")
```
Next, set up the parameter space to explore. This is where dials comes in. 
```{r}
xgboost_params <- 
  dials::parameters(
    min_n(),
    tree_depth(),
    learn_rate(),
    loss_reduction()
  )
```
Then we specify how to explore this space. From the help, "The types of designs supported here are latin hypercube designs and designs that attempt to maximize the determinant of the spatial correlation matrix between coordinates."

We are using the latter, max entropy approach. This is a way to try and make sure that the choices we make are far apart and explore a lot of the parameter space. 
```{r}
xgboost_grid <- 
  dials::grid_max_entropy(
    xgboost_params, 
    size = 60
  )
```
To see what we are gonna try, we can use this command.
```{r}
knitr::kable(head(xgboost_grid))
```
We define a workflow next. A workflow is another convenience feature. From the ever-helpful help: 

"A workflow is an object that can bundle together your pre-processing, modeling, and post-processing requests. For example, if you have a recipe and parsnip model, these can be combined into a workflow. The advantages are:

* You don’t have to keep track of separate objects in your workspace.
* The recipe prepping and model fitting can be executed using a single call to fit().
* If you have custom tuning parameter settings, these can be defined using a simpler interface when combined with tune.
* In the future, workflows will be able to add post-processing operations, such as modifying the probability cutoff for two-class models."

Seems fairly rad. Here is how to make one.
```{r}
  xgboost_wf <- workflows::workflow() %>%
      add_model(xgboost_model) %>% 
      add_formula(response_taxon ~ .)
```
Now we do some tuning. THIS IS A BIG WIN over sklearn. The Hyperopt package allows for smart hyper parameter tuning, and isn't too hard to figure out, but this is way easier. Also, getting the user-friendly hyperopt wrapper hpsklearn installed can be a bit tricky. This is better. 
```{r}
xgboost_tuned <- tune::tune_grid(
  object = xgboost_wf,
  resamples = phyllo_chopped,
  grid = xgboost_grid,
  metrics = yardstick::metric_set(rmse, rsq, mae),
  control = tune::control_grid(verbose = TRUE)
)
```
FAILING HERE, probably bc I have one level or an NA in one of my factors after splitting. 

See what worked best...
```{r}
xgboost_tuned %>%
  tune::show_best(metric = "rmse") %>%
  knitr::kable()
```
```{r}
xgboost_best_params <- xgboost_tuned %>%
  tune::select_best("rmse")
knitr::kable(xgboost_best_params)
```
Feed these best parameters into our model...
```{r}
xgboost_model_final <- xgboost_model %>% 
  finalize_model(xgboost_best_params)
```
Use code at last step to evaluate model performance
https://www.r-bloggers.com/2020/05/using-xgboost-with-tidymodels/
